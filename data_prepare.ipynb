{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37022dd6-0f98-4d23-8b74-992104ff2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20446645-aefe-4a78-b454-4ece17256cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_page_urls():\n",
    "    \"\"\"# 爬取所有2866个连接 \"\"\"\n",
    "    urls0 = [f\"https://gwins.org/cn/milesguo/list_2_{i}.html\" for i in range(1,73)]\n",
    "    urls1 = []\n",
    "    for url in tqdm(urls0):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:    \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            html_doc = soup.get_text() \n",
    "            link_string = '\\n'.join([str(link) for link in soup.find_all('a')])\n",
    "            pattern = r\"/cn/milesguo/[\\w/]+\\.html\"\n",
    "            matches = re.findall(pattern, link_string)\n",
    "            urls2 = [f\"https://gwins.org{x}\" for x in matches]\n",
    "            urls1 += urls2\n",
    "            print(len(urls1))\n",
    "        else:\n",
    "            print(f\"无法获取页面{url}，HTTP状态码：{response.status_code}\")\n",
    "    return urls1\n",
    "\n",
    "def download_documents():\n",
    "    \"\"\"# 爬取所有2866个文章，并保存为文档\"\"\" \n",
    "    urls1 = crawler_page_urls() # 爬取所有2866个连接\n",
    "    out_folder = \"./txts\"\n",
    "    if not os.path.isdir(out_folder): \n",
    "        os.mkdir(\"./txts\")\n",
    "    for url in tqdm(urls1):\n",
    "        pattern = r'\\d+'\n",
    "        id = re.search(pattern, url).group() # 获取网页编号\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:    \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        else:\n",
    "            print(f\"无法获取页面{url}，HTTP状态码：{response.status_code}\")\n",
    "            continue\n",
    "        html_doc = soup.get_text()  # 获取网页中的纯文本内容\n",
    "        html_doc = re.sub(r'\\s+', ' ', html_doc) # 去掉多余空格\n",
    "        \n",
    "        file_path = os.path.join(out_folder, f\"{id}.txt\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(html_doc) # 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74cf9050-0213-4904-b221-b74ffbb92189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_titles(input_dir=\"./txts/\", out_file=\"titles.json\"):\n",
    "    \"\"\"提取每个文档的标题，并按照编号整理到一个json文件中\"\"\"\n",
    "    files = [os.path.join(input_dir, x) for x in os.listdir(input_dir)]\n",
    "    dict1 = dict()\n",
    "    for in_file in tqdm(files):\n",
    "        id = os.path.basename(in_file).split(\".\")[0]\n",
    "        with open(in_file, \"r\") as f:\n",
    "            txt = f.read()\n",
    "        pattern1 = r'^(.*?)首页'\n",
    "        match = re.match(pattern1, txt)\n",
    "        if match: \n",
    "            title = match.group(1)\n",
    "            title = title.replace(\"\\n\", \" \")\n",
    "        else:\n",
    "            title = \"unknown title\"\n",
    "        dict1[id] = title\n",
    "        #print(title)\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump(dict1, f)\n",
    "\n",
    "def load_titles(file=\"title.json\"):\n",
    "    \"\"\"读取标题文件\"\"\"\n",
    "    with open(file, \"r\") as f:\n",
    "        dict1 = json.load(f)\n",
    "    return dict1\n",
    "    \n",
    "def load_data_to_paragraphs(file):\n",
    "    \"\"\" 将长文档分解成1000字以内短文档. \n",
    "    因为openai sentence embedding ada 002 8000 input token, 最大2000汉字\n",
    "    但是逼近2000后语义编码效果会下降\n",
    "    \"\"\"\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    pattern1 = r'^.*?内容梗概: '\n",
    "    pattern2 = r' 友情链接：Gnews \\| Gclubs \\| Gfashion \\| himalaya exchange \\| gettr \\| 法治基金 \\| 新中国联邦辞典 \\| $'\n",
    "    data = re.sub(pattern1, \"\", data)\n",
    "    data = re.sub(pattern2, \"\", data)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    txts = text_splitter.split_text(data)\n",
    "    return txts\n",
    "\n",
    "def sentence_embedding_batch(txts, id):\n",
    "    \"\"\"将list of text编码为sentence embedding 1536维\"\"\"\n",
    "    l1 = []\n",
    "    #embs = get_embeddings(txts, engine=\"text-embedding-ada-002\") # old api\n",
    "    response = openai_client.embeddings.create(input = txts, model=\"text-embedding-ada-002\")\n",
    "    response = json.loads(response.json())[\"data\"]\n",
    "    embs = [x[\"embedding\"] for x in response]\n",
    "    for i, txt in enumerate(txts):\n",
    "        label = f\"{id}-{i}\"\n",
    "        emb = embs[i]\n",
    "        l1.append((label, txt, emb))\n",
    "    return l1\n",
    "\n",
    "def encoding_file(in_file, output_dir):\n",
    "    \"\"\"将文档.txt文件分割并编码为sentence embedding，压缩保存为同名.npz文件\"\"\"\n",
    "    id = os.path.basename(in_file).split(\".\")[0]\n",
    "    out_file = os.path.join(output_dir, id+\".npz\")\n",
    "    txts = load_data_to_paragraphs(in_file)\n",
    "    packs = sentence_embedding_batch(txts, id)\n",
    "    serialized_data = pickle.dumps(packs)\n",
    "    compressed_data = gzip.compress(serialized_data)\n",
    "    with open(out_file, \"wb\") as file:\n",
    "        file.write(compressed_data)\n",
    "\n",
    "def encoding_files(input_dir = \"./txts/\", output_dir = \"./emb\"):\n",
    "    \"\"\"批量将文件夹下txt文件编码为同名 embedding文件，2866个文件需要 openai 3美元\"\"\"\n",
    "    files = [os.path.join(input_dir, x) for x in os.listdir(input_dir)]\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    for i, in_file in enumerate(tqdm(files)):\n",
    "        encoding_file(in_file, output_dir)\n",
    "\n",
    "def decoding_file(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        compressed_data = f.read()\n",
    "    decompressed_data = gzip.decompress(compressed_data)\n",
    "    l1 = pickle.loads(decompressed_data)\n",
    "    return l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b19f50-677d-4c2b-a657-03220b7ddc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2866/2866 [46:16<00:00,  1.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2866/2866 [00:06<00:00, 438.59it/s]\n"
     ]
    }
   ],
   "source": [
    "#download_documents() # 爬虫\n",
    "encoding_files(input_dir = \"./txts/\", output_dir = \"./emb\") # 编码，保存到文件\n",
    "extract_titles(input_dir=\"./txts/\", out_file=\"titles.json\") # 提取标题，保存到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce33d79-fd77-40b5-93e8-b6d19d53938a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
