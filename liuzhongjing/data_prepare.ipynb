{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cfed658-b17c-48d8-b7ba-a4c24b1508a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "data_path = \"./LiuZhongjing-All-In\"\n",
    "if not os.path.isdir(data_path):\n",
    "    os.system(\"git clone https://github.com/LiuZhongjing/LiuZhongjing-All-In.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37022dd6-0f98-4d23-8b74-992104ff2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI # API v1.2\n",
    "\n",
    "with open(\"../openai_key.txt\", \"r\") as f:\n",
    "    openai_key = f.read().strip()\n",
    "    openai_client = OpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f0f3d0e-5a29-4a62-b643-186dd5d56857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "def get_all_files_in_folder(folder_path):\n",
    "    file_list = []\n",
    "    for root, directories, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            if filename==\"README.md\": continue\n",
    "            if not filename.endswith(\".md\"): continue\n",
    "            file_list.append(os.path.join(root, filename))\n",
    "    return file_list\n",
    "\n",
    "file_list = get_all_files_in_folder(data_path)\n",
    "file_list = sorted(file_list)\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68837a9c-bcbf-4adb-8c62-6d3b7eaf839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 101/101 [00:00<00:00, 233.02it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_titles(files, out_file=\"titles.json\"):\n",
    "    \"\"\"提取每个文档的标题，并按照编号整理到一个json文件中\"\"\"\n",
    "    dict1 = dict()\n",
    "    for id, in_file in enumerate(tqdm(files)):\n",
    "        file_name = in_file.split(\"/\")[-1].split(\".\")[0]\n",
    "        with open(in_file, \"r\") as f:\n",
    "            title = f.readlines()[0]\n",
    "        dict1[file_name] = title\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump(dict1, f)\n",
    "\n",
    "def load_titles(file=\"titles.json\"):\n",
    "    \"\"\"读取标题文件\"\"\"\n",
    "    with open(file, \"r\") as f:\n",
    "        dict1 = json.load(f)\n",
    "    return dict1\n",
    "\n",
    "extract_titles(file_list, out_file=\"titles.json\")\n",
    "dict1 = load_titles(file=\"titles.json\")\n",
    "#print(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74cf9050-0213-4904-b221-b74ffbb92189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_paragraphs(file):\n",
    "    \"\"\" 将长文档分解成1000字以内短文档. \n",
    "    因为openai sentence embedding ada 002 8000 input token, 最大2000汉字\n",
    "    但是逼近2000后语义编码效果会下降\n",
    "    \"\"\"\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    txts = text_splitter.split_text(data)\n",
    "    return txts\n",
    "\n",
    "def sentence_embedding_batch(txts, file_name):\n",
    "    \"\"\"将list of text编码为sentence embedding 1536维\"\"\"\n",
    "    l1 = []\n",
    "    #embs = get_embeddings(txts, engine=\"text-embedding-ada-002\") # old api\n",
    "    response = openai_client.embeddings.create(input = txts, model=\"text-embedding-ada-002\")\n",
    "    response = json.loads(response.json())[\"data\"]\n",
    "    embs = [x[\"embedding\"] for x in response]\n",
    "    for i, txt in enumerate(txts):\n",
    "        label = f\"{file_name}-{i}\"\n",
    "        emb = embs[i]\n",
    "        l1.append((label, txt, emb))\n",
    "    return l1\n",
    "\n",
    "def encoding_file(in_file, output_dir):\n",
    "    \"\"\"将文档.txt文件分割并编码为sentence embedding，压缩保存为同名.npz文件\"\"\"\n",
    "    file_name = os.path.basename(in_file).split(\".\")[0]\n",
    "    out_file = os.path.join(output_dir, id+\".npz\")\n",
    "    txts = load_data_to_paragraphs(in_file)\n",
    "    packs = sentence_embedding_batch(txts, file_name)\n",
    "    serialized_data = pickle.dumps(packs)\n",
    "    compressed_data = gzip.compress(serialized_data)\n",
    "    with open(out_file, \"wb\") as file:\n",
    "        file.write(compressed_data)\n",
    "\n",
    "def encoding_files(files, output_dir=\"./emb\"):\n",
    "    \"\"\"批量将文件夹下txt文件编码为同名 embedding文件，2866个文件需要 openai 3美元\"\"\"\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    for i, in_file in enumerate(tqdm(files)):\n",
    "        encoding_file(in_file, output_dir)\n",
    "\n",
    "def decoding_file(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        compressed_data = f.read()\n",
    "    decompressed_data = gzip.decompress(compressed_data)\n",
    "    l1 = pickle.loads(decompressed_data)\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36b19f50-677d-4c2b-a657-03220b7ddc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 101/101 [04:58<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "encoding_files(file_list, output_dir = \"./emb\") # 编码，保存到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce33d79-fd77-40b5-93e8-b6d19d53938a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
