{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cfdfad-283b-4f36-9479-f5795d1a5227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "from openai import OpenAI # API v2.0\n",
    "#from openai.embeddings_utils import get_embedding, get_embeddings # before v1.2\n",
    "\n",
    "with open(\"openai_key.txt\", \"r\") as f:\n",
    "    openai_key = f.read().strip()\n",
    "    openai_client = OpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27204161-33ff-467f-b6cc-7b315fa17aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数\n",
    "\n",
    "def crawler_page_urls():\n",
    "    \"\"\"# 爬取所有2866个连接 \"\"\"\n",
    "    urls0 = [f\"https://gwins.org/cn/milesguo/list_2_{i}.html\" for i in range(1,73)]\n",
    "    urls1 = []\n",
    "    for url in tqdm(urls0):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:    \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            html_doc = soup.get_text() \n",
    "            link_string = '\\n'.join([str(link) for link in soup.find_all('a')])\n",
    "            pattern = r\"/cn/milesguo/[\\w/]+\\.html\"\n",
    "            matches = re.findall(pattern, link_string)\n",
    "            urls2 = [f\"https://gwins.org{x}\" for x in matches]\n",
    "            urls1 += urls2\n",
    "            print(len(urls1))\n",
    "        else:\n",
    "            print(f\"无法获取页面{url}，HTTP状态码：{response.status_code}\")\n",
    "    return urls1\n",
    "\n",
    "def download_documents():\n",
    "    \"\"\"# 爬取所有2866个文章，并保存为文档\"\"\" \n",
    "    urls1 = crawler_page_urls() # 爬取所有2866个连接\n",
    "    out_folder = \"./txts\"\n",
    "    if not os.path.isdir(out_folder): \n",
    "        os.mkdir(\"./txts\")\n",
    "    for url in tqdm(urls1):\n",
    "        pattern = r'\\d+'\n",
    "        id = re.search(pattern, url).group() # 获取网页编号\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:    \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        else:\n",
    "            print(f\"无法获取页面{url}，HTTP状态码：{response.status_code}\")\n",
    "            continue\n",
    "        html_doc = soup.get_text()  # 获取网页中的纯文本内容\n",
    "        html_doc = re.sub(r'\\s+', ' ', html_doc) # 去掉多余空格\n",
    "        \n",
    "        file_path = os.path.join(out_folder, f\"{id}.txt\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(html_doc) # 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe45e3a-476f-4ea0-adc7-6f3af655701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_titles(input_dir=\"./txts/\", out_file=\"titles.json\"):\n",
    "    \"\"\"提取每个文档的标题，并按照编号整理到一个json文件中\"\"\"\n",
    "    files = [os.path.join(input_dir, x) for x in os.listdir(input_dir)]\n",
    "    dict1 = dict()\n",
    "    for in_file in tqdm(files):\n",
    "        id = os.path.basename(in_file).split(\".\")[0]\n",
    "        with open(in_file, \"r\") as f:\n",
    "            txt = f.read()\n",
    "        pattern1 = r'^(.*?)首页'\n",
    "        match = re.match(pattern1, txt)\n",
    "        if match: \n",
    "            title = match.group(1)\n",
    "            title = title.replace(\"\\n\", \" \")\n",
    "        else:\n",
    "            title = \"unknown title\"\n",
    "        dict1[id] = title\n",
    "        #print(title)\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump(dict1, f)\n",
    "\n",
    "def load_titles(file=\"title.json\"):\n",
    "    \"\"\"读取标题文件\"\"\"\n",
    "    with open(file, \"r\") as f:\n",
    "        dict1 = json.load(f)\n",
    "    return dict1\n",
    "    \n",
    "def load_data_to_paragraphs(file):\n",
    "    \"\"\" 将长文档分解成1000字以内短文档. \n",
    "    因为openai sentence embedding ada 002 8000 input token, 最大2000汉字\n",
    "    但是逼近2000后语义编码效果会下降\n",
    "    \"\"\"\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    pattern1 = r'^.*?内容梗概: '\n",
    "    pattern2 = r' 友情链接：Gnews \\| Gclubs \\| Gfashion \\| himalaya exchange \\| gettr \\| 法治基金 \\| 新中国联邦辞典 \\| $'\n",
    "    data = re.sub(pattern1, \"\", data)\n",
    "    data = re.sub(pattern2, \"\", data)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    txts = text_splitter.split_text(data)\n",
    "    return txts\n",
    "\n",
    "def sentence_embedding_batch(txts, id):\n",
    "    \"\"\"将list of text编码为sentence embedding 1536维\"\"\"\n",
    "    l1 = []\n",
    "    #embs = get_embeddings(txts, engine=\"text-embedding-ada-002\") # old api\n",
    "    response = openai_client.embeddings.create(input = txts, model=\"text-embedding-ada-002\")\n",
    "    response = json.loads(response.json())[\"data\"]\n",
    "    embs = [x[\"embedding\"] for x in response]\n",
    "    for i, txt in enumerate(txts):\n",
    "        label = f\"{id}-{i}\"\n",
    "        emb = embs[i]\n",
    "        l1.append((label, txt, emb))\n",
    "    return l1\n",
    "\n",
    "def encoding_file(in_file, output_dir):\n",
    "    \"\"\"将文档.txt文件分割并编码为sentence embedding，压缩保存为同名.npz文件\"\"\"\n",
    "    id = os.path.basename(in_file).split(\".\")[0]\n",
    "    out_file = os.path.join(output_dir, id+\".npz\")\n",
    "    txts = load_data_to_paragraphs(in_file)\n",
    "    packs = sentence_embedding_batch(txts, id)\n",
    "    serialized_data = pickle.dumps(packs)\n",
    "    compressed_data = gzip.compress(serialized_data)\n",
    "    with open(out_file, \"wb\") as file:\n",
    "        file.write(compressed_data)\n",
    "\n",
    "def encoding_files(input_dir = \"./txts/\", output_dir = \"./emb\"):\n",
    "    \"\"\"批量将文件夹下txt文件编码为同名 embedding文件，2866个文件需要 openai 3美元\"\"\"\n",
    "    files = [os.path.join(input_dir, x) for x in os.listdir(input_dir)]\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    for i, in_file in enumerate(tqdm(files)):\n",
    "        encoding_file(in_file, output_dir)\n",
    "\n",
    "def decoding_file(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        compressed_data = f.read()\n",
    "    decompressed_data = gzip.decompress(compressed_data)\n",
    "    l1 = pickle.loads(decompressed_data)\n",
    "    return l1\n",
    "\n",
    "def build_faiss_index(embs):\n",
    "    d = 1536\n",
    "    nlist = 100\n",
    "    index = faiss.IndexFlatIP(d) # 普通向量内积暴力检索索引\n",
    "    #index = faiss.IndexIVFFlat(index, d, nlist) # 倒排索引\n",
    "    index.train(embs)\n",
    "    index.add(embs)\n",
    "    return index\n",
    "\n",
    "def build_veactor_search_index(folder=\"./emb\"):\n",
    "    \"\"\"构建向量检索索引和字典，充当向量数据库功能\"\"\"\n",
    "    files = [os.path.join(folder, x) for x in os.listdir(folder)]\n",
    "    dict_emb = dict()\n",
    "    i = 0\n",
    "    embs = []\n",
    "    for file in tqdm(files):\n",
    "        l1 = decoding_file(file)\n",
    "        for idx, txt, emb in l1:\n",
    "            dict_emb[i] = {\"idx\": idx, \"txt\": txt, \"emb\": emb}\n",
    "            embs.append(emb)\n",
    "            i+=1\n",
    "    embs = np.vstack(embs)\n",
    "    embs /= np.linalg.norm(embs, ord=2, axis=-1, keepdims=True) + 1e-8 # L2归一化\n",
    "    faiss_index = build_faiss_index(embs)\n",
    "    return embs, dict_emb, faiss_index\n",
    "\n",
    "def text_search(query, faiss_index, dict_emb, dict_title, k=3):\n",
    "    \"\"\" \n",
    "    用query文本，访问openai sentence embedding ada 002，得到句向量\n",
    "    每1000个token(250汉字) 0.0001美元\n",
    "    在向量索引中检索语音最相近的文档，并找对对应标题。\n",
    "    \"\"\"\n",
    "    if len(query)<10:\n",
    "        query = f\"这是一个关于{query}的句子\"\n",
    "    #emb_query = get_embedding(query, engine=\"text-embedding-ada-002\")\n",
    "    response = openai_client.embeddings.create(input=[query], model=\"text-embedding-ada-002\")\n",
    "    emb_query = json.loads(response.json())[\"data\"][0][\"embedding\"]\n",
    "    emb_query = np.array(emb_query).reshape((1, -1))\n",
    "    D, I = faiss_index.search(emb_query, k)\n",
    "    txts = []\n",
    "    for i in I[0]:\n",
    "        idx = dict_emb[i][\"idx\"]\n",
    "        idx0 = idx.split(\"-\")[0]\n",
    "        txt = dict_emb[i][\"txt\"]\n",
    "        title = dict_title[idx0]\n",
    "        txts += [(title, txt)]\n",
    "    return txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c029e02b-378d-4450-b77c-b5fe7c3d9b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2866/2866 [00:06<00:00, 465.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#download_documents() # 爬虫\n",
    "#encoding_files(input_dir = \"./txts/\", output_dir = \"./emb\") # 编码，保存到文件\n",
    "#extract_titles(input_dir=\"./txts/\", out_file=\"titles.json\") # 提取标题，保存到文件\n",
    "embs, dict_emb, faiss_index = build_veactor_search_index(folder=\"./emb\") # 读取编码文件，构建向量索引\n",
    "dict_title = load_titles(file=\"titles.json\") # 读取标题文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "45c982ce-049a-4264-a8b3-68c3b312213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' 郭文贵2022年5月17日直播 20220517_1直播乱聊 ',\n",
       "  '但是王岐山就选择李克强那边，应该是没想到吧。王岐山和曾全走到李克强那边去了，这是江和曾和这个共青团，绝对是水火不容啊是吧？李克强是胡锦涛的人啊，而且习也是恨这个李克强，但你能想到吗现在？王岐山、曾庆红通通地走到李克强那去 就连朱镕基，当年朱镕基当时当总理的时候，李克强只要一说话，只要一说话，朱镕基就骂他。就朱镕基欺负他，欺负到了要死的程度。现在朱镕基也投靠李克强了，你想想这是什么概念啊？兄弟姐妹们。 所以说这个问题，它已经不是政治的问题啦，它是一个生死的选择。大家想想，就是抓个稻草，但是稻草也是草，我觉得李克强连个稻草都不算。我觉得李克强连稻草都不算，现在共产党抓了一个连稻草都不算的李克强。 （看留言）李克强的接任可能性有多大？可能性有多大是吧？没什么可能性我觉得。谢谢Rachel！ 小王子：七哥那您说就现在整个20大前夕王岐山还有朱镕基、曾庆红他们整个投靠李克强，这个反映了党内现在政治斗争怎么样的一个情况？ 郭文贵先生：我觉得李克强是要绝对玩政治上绝不输习近平的，这我绝对有说话权力。但是李克强现在是一个没有选择下的一个这些人的选择。但是我觉得到最后的时候，可能估计都不会让李克强上去，如果有那天的话——习被灭的话，我会觉得李克强他腿没迈出去就给灭了就。 我觉得现在已经没有什么”上海帮“啦，现在严格讲曾家是吧，还有这个朱镕基家，所有这些人，谢谢！（工作人员取走水杯）所有这些人都是在保命，谁管你什么共产党不共产党。如果说现在说把共产党贬成猪党、狗党他也愿意，只要能保命，就是保命嘛。现在跟七哥当年说的一样保命是吧？保财、报仇，这都是所有人的心里想法，跟共产党已经没有半毛钱关系了。 但是人家会利用共产党是吧，利用这个组织和权力，就是抢的这个权力嘛。我觉得李克强是被所有人利用的一个人，但是最后我不相信他，有机会，很少的机会，很少。李克强强硬一下，李克强根本没有强硬，李克强做最后的垂死挣扎。 习绝对是有高人在帮他运作，就让他干。 郭文贵先生：好，咱今天试直播试到这啊，现在的感觉怎么样？我觉得很好，就那天我们直播的时候你看，现在已经是黑了吗？ 小王子：看一下有没有反光主——镜头？ 工作人员：稍稍有些。 郭文贵先生：如果那天，现在是几点？七点多吗？八点了都，那就太没问题啦！你干嘛要搞，就这样就行了嘛。 小王子：我们可以一会儿继续，七哥您忙先走，我们可以继续直播到晚上。'),\n",
       " (' 郭文贵2022年2月27日直播 20220227_1俄乌战争预测台海；普京嚣张中共角色；世界经济金融市场；台湾沦陷如何应对 ',\n",
       "  '这不说战争结果，别说被干掉还是干不干掉，先别说，剩下就两个奇怪的结局：一，和解了，大家都回去，它也要面对这个结局；二，它放核武器，或者是乌克兰和美国在哪方面再支持它，把它给整怎么着了，然后它再来个狠的，什么“切尔诺贝利泄漏”或者什么“普京被那边被做掉”，这种滑稽的结局一定会出现的，突然间，会突然间。一方被干掉，一方停战，被改变，或哪个人再出事，但是俄罗斯的命运不会改变的，就是这结局了。 那共产党的结局是什么你告诉我？从今天起，一会儿你们谈谈，到今天为止，共产党会面临什么样的结局啊？咱一会儿说，谢谢好好先生！你第一天今天上来就告诉我，你在美国把护照弄掉了回去，你这家伙，我见了你我非踹你两脚不行，你个傻子，被他们给骗，咱要报仇！谢谢好好先生！ 好好先生：好的，谢谢七哥！我们一定会报仇的！那媒体战这一边也是战争，我们一定会智取。好的，谢谢，交给主持人。 卡丽熙：谢谢七哥！七哥一席话让我们知道、更加知道这个战略的智慧和信息时代的战争。其实我还想说一句，我们好好先生虽然一开始被在美国放弃了，但是我们片头的这个歌曲是我们好好先生唱的，他是一个多才多艺的年轻人。 郭文贵先生：是吗？特别棒！弹得比我好，弹的比我好。你现在是关键以乐灭共，加入到唐平的队伍里面去，好好先生，赶快找唐平，太关键了，谢谢了。 卡丽熙：好，七哥，我们现在是继续提问题还是让我们的视频组跟大家分享PPT？我们七哥累了，要不要喝点水、喝点茶 郭文贵先生：可以呀，可以呀，PPT，分享PPT。 卡丽熙：好，请我们的导播请出我们的Eric给大家分享我们的今天的PPT。 郭文贵先生：又是Eric来了（注：笑）Eric我的兄弟，太可爱了，超级猛男。 1:00:07Eric分享PPT《俄乌谈判破裂 世界进入危局》 卡丽熙：非常感谢我们的Eric。刚才我们的七哥跟我们分享了很多很多的信息时代的战略智慧。七哥来了，好，还给七哥。我们刚才Eric分享了我们的PPT，七哥，有什么跟大家要分析的吗？ 1:03:22 Eric很优秀，男人有才华就可爱；女人善良就漂亮'),\n",
       " (' 郭文贵2022年10月23日直播 20221023_2 【中共伪二十大事件真相 】 ',\n",
       "  '李克强看着这个时候啊，大家你看到李克强从这个之前一直往前看，根本不敢侧看，此人怎么能当总理呢？！我曾经跟他比，跟你火来比，跟飞飞还接近过。我们在一起在相枕而眠七整天，又在一起一个屋子睡觉不超过--最起码得40个夜晚，是吧。我们跟你们没有睡过，一个人睡过一整晚觉。就这个人，我从第一天，我就说这个家伙我看不起他，我就真的看不起他。我今天我真的是......此人不是人也，也就甭说是男人了。 你看啊，你看这个可怜，“我就想说几句，”你看到后面的人，你看到王毅，你看到了解放军将领，你想想这些，中国这些还有一点良知吗，中国人？！黑道儿都不会这样。你看看，你看看这个汪洋，那个汪洋在和王岐山，一会儿要找到另一个角度视频，小铁。他的动作是最大的，就汪洋和韩正和王岐山是这里最想有动作的。你可以看到几次想起身，眼睛很一直往下看，一直有没有人说话，有没有人说句话呀，都想等着别人开口。 嗯，就没有像当年罗马尼亚齐奥塞斯库演讲的时候，有个角落里喊，“打倒齐奥塞斯库”，旁边就是“打倒齐奥塞斯库”，完了！齐奥塞斯库被枪毙了。就没有这一嗓子，悲哀！！强大的中国人民实在太强大了，我看完这个以后，我在想真的这个灭共的事情，咱要不要灭共，真的，还灭它干啥呀，就让共产党那儿多待几年，就这帮王八蛋互相杀去吧。 你看看这种德性，你觉得哪个人配当我们的同类，同族？！我们感到羞耻不羞耻？！你再看看海外的所有的欺民贼，还有伪类们，就在美国，所有人把胡锦涛说成是有病，他今天的身体状态比他退出还好，他也没有帕金斯症，他脑子倍儿清楚，而且能写，我很惊讶。 我昨天才知道，我说，“他到底写的啥？”他说他写的：‘他是支持习近平在特殊情况下连任第三期，但下不为例，最后一期。然后要继续搞党内民主，反对隔离清零，对老百姓太惨了，继续经济改革开放，然后坚决不能打台湾，对国家统一最好的办法就不要打台湾，承诺不打台湾。’你说这个脑子是清楚的，是吧。 这种情况下，他自己写，弄了一页纸，他是准备好，今天你选完我，我讲几句话，习近平也答应他了。但是他到这里一看你这个名字了，他就不愿意了，他要抢习的麦克风，麦克风，他就推他，推他的时候，这时候习就就把孔绍逊找来，保镖找来，给我架出去。 他俩来了就要骗他，“我让你站前面演讲去，”就给绑架走。旁边的王沪宁，栗战书执行了这个惨无人道的踩脚，拖走，没收文件。韩正和汪洋和王岐山跃跃欲试，但最终没敢出手。请。')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_query = \"李克强的结局会是怎样？\"\n",
    "txts_retrival = text_search(txt_query, faiss_index, dict_emb, dict_title, k=3) #请求10000次API成本1美金\n",
    "txts_retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee45a8c9-ad4a-462d-98d1-1323799d9f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "郭文贵是一名中国人，被广泛认为是一个争议性的人物。他公开反对中国共产党，并通过社交媒体和其他渠道发表大量反对政府的言论。他声称自己因政治原因受到监禁，并强调了对中国政府的不满情绪。尽管郭文贵一直在公开场合中批判中共的体制和政策，但也造成了很多关于他本人的争议性传言，包括一些负面的诽闻和指控。\n",
      "以下是检索得到的原始参考文本\n",
      "1.  郭文贵2018年11月20日视频 20181120_4路德访谈班农、文贵先生：谈谈未来制裁中共王岐山盗国贼的一系列行为 \n",
      "郭文贵先生：这个不知道，听说很多人被抓了，他们去查，抓很多人，说谁把这个水立方的1120给打出来了。我们好几个员工也被抓走了，怀疑我们。 路德：就为这事？ 郭文贵先生：我说跟我们什么关系啊？还有一个你注意到了吗？就在我们开始演讲的前20分钟，班农和郭文贵一下子网上释放了，就在我们讲完，就会议刚刚进行完…… 路德：还专门发了个推。 郭文贵先生：是吗？我没有时间（看），刚刚讲完马上又封掉了。什么概念？说明了党内的99的九千万党员和99.999的人民希望就那几个人完蛋。 Sarah：对。 郭文贵先生：中国今天所有的问题你们都要问问十八大以后的政府，我们要问问习近平，我们要问问王岐山，谁都不要问。 当然你孟建柱、傅政华、孙力军，你就是替他咬人的嘛。那么，班农先生和律师团队、你知道和我说什么？我特别、当时几次问我，（我）一句话没说。因为他们外国人不会跟你想的，跟你虚虚实实，他都是来真的，说：“郭先生，我们律师为什么要放弃你这个？放弃你这个？除了香港我和你有利益冲突，代表着可能与海航有关系，我不能代表你。更重要的事情，所有人都告诉我们说 ‘你是习近平的一条狗，’ running dog，你是有政党冲突的关键人物，我们不想裹入这个政治。在新闻发布会以后，我继续代表你，所有的subpoena我帮你发上去，但这个会之前，我不可以。” 什么意思？人家共产党的宣传就是造谣：郭文贵是习近平的一条狗，是为了咬王岐山的。 但我今天，我8次提到习近平，我说：“习近平你知不知道海航这个钱移来移去，谁是背后的股东？你为什么不调查？” 我不反习近平，我不等于说我今天承认你呀！我从来没有承认过他呀，我也对他抱有希望啊，现在我全是失望啊。 Sarah：对。 郭文贵先生：我的家人在监狱里，我的钱被罚款，简直疯狂了！香港的法官、律政司都被他们威胁！律政司的人，对不起，又说漏了。（律政司的人）给我发信息：“对不起，郭先生，有一天我到美国去，我到法院给你作证，我受到了共产党的威胁，让我来协调法官。而且有一个香港巨大的一个富豪案子，是让我把人给放掉，不允许判，而且指定我什么时候开庭，什么时候…我要跟你在一起。”\n",
      "2.  郭文贵2023年2月15日直播 20230215_1AI和ChatGPT对美国和台湾的威胁可能比核武器还严重 \n",
      "好，（读ChatGPT的回答）郭文贵是一个争议的人物，一直被卷入各种争议之中，关于他的谣言指控也是层出不穷，网上流传的其中一项指控是他性能力很差，性力只能持续三秒钟，目前还不清楚这谣言是从哪里开始的，也不清楚它是否属实。请务必注意，散布谣言或对某人的私生活做出毫无根据的指挥可能会造成伤害和破坏。通常不宜从事此类行为。此外，批评缺乏可靠证据或来源的谣言和指挥也很重要。一般而言，需要的是批判地评估信息和声明，并在寻找有关个人或事件的信息时依赖信誉良好的来源。应对谣言和未经证实的说法持谨慎和怀疑态度。 哇塞，好，现在开始了照片。要找几个七哥的照片贴上去，找几个习近平的照片贴上去，看它啥反应？对了，链接打开，它啥反应，随便搞两下子，你就能证明…… （郭先生手机联系要歌词）磐石农场、唐平妹妹、Qmay，谁有这个《沧海一声笑》的中文歌词，咱们的版本。 那就不要，那就不要，小视频能描述吗？这绝对是共产党，还有这世界上的寡头、巨头们对人类的新的一个洗脑的控制的一个利器，我称它为精神界的疫苗，就是社交媒体的新疫苗，就是打了让你死，不打也让你死。只要你敢沾，我就是死。ChatGPT绝对是处心积虑的AI思想武器，AI疫苗。100%的，大家记住，千万不要上ChatGPT的当，ChatGPT100%是大佬和政治。说白了，给老百姓没一点好事，对老百姓没有半点好处，就是要用你的时间，让你当它ChatGPT的奴隶，为它赚钱，为它收集信息，然后洗你的脑，拿走你的未来，毁灭毁掉你的一切。只要不是基于区块链，不是基于一个完全独立的，不是以利为目的的，都是骗子。兄弟姐妹们，一定要记住，这是社交媒体的疫苗，这是一个灾难。AI时期的黑暗时刻。 这是小皮匠给我发来的，这叫小皮匠干事情，就是发了一个去掉头的，滔滔灭共潮，四海英雄 聚今朝。这个是，这个是，这是啥版的？ 滔滔灭共潮，你看看这，她上哪找这版本。四海英雄，四海英雄聚今朝。（郭先生唱《沧海一声笑》）“苍天笑，拔剑斩红妖，浪沙淘尽毒霾消江山”这是啥词儿？“江山笑，天宇摇”这在哪找词的小皮匠？我都没唱过这词。 哎，这是什么意思，小铁兄弟。（郭先生看ChatGPT问题回答）你看到没有，所以说这帮孙子现在已经是，就是问郭文贵，答非郭文贵。郭文贵的三秒这个是真的，这是真的。哎呀，这是真的，这是真的。\n",
      "3.  郭文贵2021年11月12日直播 20211112_1中共六中全会的谎言给世界和新中国联邦带来了什么；在新金融时代，新国际大秩序，全世界消灭共产主义的时机中，如何智慧理财投资；新中国联邦将建立安保国防系统，保卫世界华人人身财产 \n",
      "00:00 片头视频：郭文贵先生视频片段 1、保命保钱还要报仇！ 2、我站出来必须爆料，那么这就是为什么我说要把王岐山这个书记这个事情直接给说出来的原因嘛。 3、我是冒着我全家一千多亿的资产，我全家人被抓，员工被抓被杀。我在时刻面临生命危险，我就是为了维权吗？ 4、用一国之力去对一个人制造这样的谣言前所未有。 5、29年来，文贵一分一秒没有犹豫过。文贵从来没有想过放弃，既然已经开始。我们就绝不妥协。既然已经开始，我们一定要赢， 6、忘掉文贵，过去文贵已经早就死了，现在的文贵是承负着上天给我的使命。 7、喜马拉雅是一个高度，是一个我们的目标，目标要让中国有法制中国，信仰中国。 8、我们每个人生来平等，我们每个人都应该有同等的机会，每个人都应该得到尊重，我们必须赢。该我们赢啦！民心向郭，习亡岐山。 9、在香港你所有做的恶，你都要万倍地偿还。 10、CCP是武汉的这个冠状病毒，就是共产党生物实验室P4实验室（制造的） 11、这个美国所推出的辉瑞制药的疫苗，这绝对是共产党的病毒的再次的次生灾难。这是黑暗势力和共产党势力勾结的结果。 03：49 直播开始，互致问候 Rachel：全球的战友们，大家好！ 郭文贵先生：Rachel！ Rachel：哎！尊敬的战友大家好！很高兴今天又在11月12日和文贵先生还有我们的全球的战友在这里大直播。那我们今天的节奏会快一点，我先让我们的在出屏的战友先跟大家打声招呼，我们从我们的这个唐平先开始。 唐平：七哥好！文紫好！Rachel好！宁南好！还有全球的战友们好！很开心能够上大直播，没有迟到。呵呵呵。 郭文贵先生：呵呵呵呵呵呵，哎我还忘了这事儿呢，我都忘了你俩上回迟到，提醒我啦啊，你把他俩踢出去，快点儿，呵呵呵，呵呵呵呵。开玩笑开玩笑，我都忘了这事儿，你还记着呢，挺好。 威廉王：七哥好！嘉宾好！所有的战友们好！非常荣幸能上我们的大直播，谢谢！ Rachel：我就按顺序来，接下来所有的回答都是唐平、威廉王，然后文紫、然后宁南啊。文紫。 文紫：好，太激动啦！七哥好！主持人好！唐平还有威廉王好！还有这位宁南博士好！非常的激动，今天是我第一次上大直播呀，坦白地讲，本来只是打个招呼啊，那我先打个招呼吧，好。\n"
     ]
    }
   ],
   "source": [
    "def RAG_chatbot(txt_query, txts_retrival):\n",
    "    \"\"\"\n",
    "    k=3时token 4000左右，请求250次API成本1美金左右，成本和输入token数量，即和检索数量k成正比\n",
    "    gpt-3.5-turbo-1106，是最新版3.5模型，最大token 16k\n",
    "    需要试验其他开源LLM\n",
    "    reference： https://openai.com/pricing#language-models\n",
    "    \"\"\"\n",
    "    prompt = \"\\n\\n\".join([f\"标题：{title}\\n 正文：{txt}\" for title, txt in txts_retrival])\n",
    "    prompt = f\"你需要先摘要并总结参考文本，再解答这个问题{txt_query} 以下是参考文本\\n\\n{txts_retrival}\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
    "      messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    txt_response = json.loads(response.json())[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return txt_response\n",
    "\n",
    "txt_query = \"郭文贵是谁？\"\n",
    "txts_retrival = text_search(txt_query, faiss_index, dict_emb, dict_title, k=3)\n",
    "txt_response = RAG_chatbot(txt_query, txts_retrival)\n",
    "print(txt_response)\n",
    "print(\"以下是检索得到的原始参考文本\")\n",
    "for i, (title, txt) in enumerate(txts_retrival):\n",
    "    print(f\"{i+1}. {title}\")\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d836582c-8c40-40a7-89ba-c7245ee622f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
