{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8cfdfad-283b-4f36-9479-f5795d1a5227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, get_embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"openai_key.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27204161-33ff-467f-b6cc-7b315fa17aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取所有2866个连接\n",
    "urls0 = [f\"https://gwins.org/cn/milesguo/list_2_{i}.html\" for i in range(1,73)]\n",
    "urls1 = []\n",
    "for url in tqdm(urls0):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:    \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        html_doc = soup.get_text() \n",
    "        link_string = '\\n'.join([str(link) for link in soup.find_all('a')])\n",
    "        pattern = r\"/cn/milesguo/[\\w/]+\\.html\"\n",
    "        matches = re.findall(pattern, link_string)\n",
    "        urls2 = [f\"https://gwins.org{x}\" for x in matches]\n",
    "        urls1 += urls2\n",
    "        print(len(urls1))\n",
    "    else:\n",
    "        print(f\"无法获取页面{url}，HTTP状态码：{response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73a330-493e-4cea-b3e6-2ae7f8da2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取所有2866个文章，并保存为文档\n",
    "out_folder = \"./txts\"\n",
    "if not os.path.isdir(out_folder): \n",
    "    os.mkdir(\"./txts\")\n",
    "for url in tqdm(urls1):\n",
    "    pattern = r'\\d+'\n",
    "    id = re.search(pattern, url).group() # 获取网页编号\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:    \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    else:\n",
    "        print(f\"无法获取页面{url}，HTTP状态码：{response.status_code}\")\n",
    "        continue\n",
    "    html_doc = soup.get_text()  # 获取网页中的纯文本内容\n",
    "    html_doc = re.sub(r'\\s+', ' ', html_doc) # 去掉多余空格\n",
    "    \n",
    "    file_path = os.path.join(out_folder, f\"{id}.txt\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(html_doc) # 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cbe45e3a-476f-4ea0-adc7-6f3af655701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将长文档分解成1000字以内短文档. 因为openai sentence embedding ada 002 8000 input token, 2000汉字\n",
    "def load_data_to_paragraphs(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    pattern1 = r'^.*?内容梗概: '\n",
    "    pattern2 = r' 友情链接：Gnews \\| Gclubs \\| Gfashion \\| himalaya exchange \\| gettr \\| 法治基金 \\| 新中国联邦辞典 \\| $'\n",
    "    data = re.sub(pattern1, \"\", data)\n",
    "    data = re.sub(pattern2, \"\", data)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    txts = text_splitter.split_text(data)\n",
    "    return txts\n",
    "\n",
    "def sentence_embedding_batch(txts, id):\n",
    "    \"\"\"将list of text编码为sentence embedding 1538维\"\"\"\n",
    "    l1 = []\n",
    "    embs = get_embeddings(txts, engine=\"text-embedding-ada-002\")\n",
    "    for i, txt in enumerate(txts):\n",
    "        label = f\"{id}-{i}\"\n",
    "        emb = embs[i]\n",
    "        l1.append((label, txt, emb))\n",
    "    return l1\n",
    "\n",
    "def encoding_file(in_file, output_dir):\n",
    "    \"\"\"将文档.txt文件编码为同名 embedding文件\"\"\"\n",
    "    id = os.path.basename(in_file).split(\".\")[0]\n",
    "    out_file = os.path.join(output_dir, id+\".npz\")\n",
    "    txts = load_data_to_paragraphs(in_file)\n",
    "    packs = sentence_embedding_batch(txts, id)\n",
    "    serialized_data = pickle.dumps(packs)\n",
    "    compressed_data = gzip.compress(serialized_data)\n",
    "    with open(out_file, \"wb\") as file:\n",
    "        file.write(compressed_data)\n",
    "\n",
    "def encoding_files(input_dir = \"./txts/\", output_dir = \"./emb\"):\n",
    "    \"\"\"批量将文件夹下txt文件编码为同名 embedding文件\"\"\"\n",
    "    files = [os.path.join(input_dir, x) for x in os.listdir(input_dir)]\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    for i, in_file in enumerate(tqdm(files)):\n",
    "        encoding_file(in_file, output_dir)\n",
    "\n",
    "def decoding_file(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        compressed_data = f.read()\n",
    "    decompressed_data = gzip.decompress(compressed_data)\n",
    "    l1 = pickle.loads(decompressed_data)\n",
    "    return l1\n",
    "\n",
    "def build_faiss_index(embs):\n",
    "    import faiss\n",
    "    d = 1536\n",
    "    nlist = 100\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    #index = faiss.IndexIVFFlat(index, d, nlist)\n",
    "    index.train(embs)\n",
    "    index.add(embs)\n",
    "    return index\n",
    "    \n",
    "\n",
    "def build_veactor_search_index(folder=\"./emb\"):\n",
    "    files = [os.path.join(folder, x) for x in os.listdir(folder)]\n",
    "    dict1 = dict()\n",
    "    i = 0\n",
    "    embs = []\n",
    "    for file in tqdm(files):\n",
    "        l1 = decoding_file(file)\n",
    "        for idx, txt, emb in l1:\n",
    "            dict1[i] = {\"idx\": idx, \"txt\": txt, \"emb\": emb}\n",
    "            embs.append(emb)\n",
    "            i+=1\n",
    "    embs = np.vstack(embs)\n",
    "    embs /= np.linalg.norm(embs, ord=2, axis=-1, keepdims=True) + 1e-8\n",
    "    faiss_index = build_faiss_index(embs)\n",
    "    return embs, dict1, faiss_index\n",
    "\n",
    "def text_search(query, faiss_index, dict1):\n",
    "    if len(query)<10:\n",
    "        query = f\"这是一个关于{query}的句子\"\n",
    "    emb_query = get_embedding(query, engine=\"text-embedding-ada-002\")\n",
    "    emb_query = np.array(emb_query).reshape((1, -1))\n",
    "    D, I = faiss_index.search(emb_query, k=3)\n",
    "    print(D, I)\n",
    "    txts = [(dict1[i][\"idx\"], dict1[i][\"txt\"]) for i in I[0]]\n",
    "    return txts\n",
    "\n",
    "def RAG_chatbot(txt, faiss_index, dict1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c029e02b-378d-4450-b77c-b5fe7c3d9b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2866/2866 [00:04<00:00, 679.97it/s]\n"
     ]
    }
   ],
   "source": [
    "#encoding_files(input_dir = \"./txts/\", output_dir = \"./emb\")\n",
    "embs, dict1, faiss_index = build_veactor_search_index(folder=\"./emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee45a8c9-ad4a-462d-98d1-1323799d9f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81078804 0.80657005 0.8030929 ]] [[18169 31816 12377]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('171-5',\n",
       "  '更没有人关心他怎么去了法国去了南法，为什么不坐他的787而换了飞机？为什么临时到了酒店，临时又换酒店还换房间？为什么从过去的十几个人现在变成了七个人而不是六个人？为什么中间有一个人离开了团队？为什么现场一个照片一个视频没有？而且是内脏死，摔死没有任何的外伤，没有什么脑壳崩裂、外伤，没有人去谈这些细节。 更夸张的事情，在同一时间内，整个国内所有的媒体、放开的公布信息和所有的和王岐山、孟建柱有关的新闻突然大变样，中美贸易战都被放在了后面，就是“泼墨门”。 大家往前看，过去的几十年来围绕着中国一系列的具有重大意义的事件，总会发生让大家感觉到所有的海外所谓的大V们、公知们、民主民运分子们、有修养有文化的这些呼吁中国有法治中国、民主中国的这些精神领袖们、主席们、总统们、法官们，还有在天空中飘的白衣们（注：袁红兵又被称作袁白衣）都会围绕着那个核心事件。 这就是我告诉西方的所有的媒体的朋友，如果你们想要了解中国，你们要从过去的这29年中国的媒体运动关键时间点所发生的海内外媒体报道它的一致性和同质性和它的标准和它的重点和真相进行了解，那是了解中国最好的方式。包括我们在海外的几家媒体、几个主持人、几个老板，拿了黑钱瞪着眼睛说瞎话，瞪着眼睛在那块转移视线。 这就是这几天发生的海航事件、709律师事件，让我们所有人都应该学习和看到我们处在了一个什么样的环境，这个世界正在被黑暗所笼罩，几乎没有讲出真相的机会，一层又一层的特务网，一层又一层的“蓝金黄”，中国14亿人民已经被牢牢的被各种利益和盗国贼所绑架。 接下来大家会看到有关这些事情戏剧性的发展。这就像在两个月以前，五月份的时候我就说中美关系将出现重大事件，大家都看到了一系列的事件发生：中兴事件，中美贸易战的谈判，几次的反复前所未有。 但是大家看看有几个海外的媒体如实报道、认真报道、认真跟踪了？这里面的意义是如此之巨大，又有多少人去报道？但是这些人所有人讲的是什么？“泼墨门”、什么某某大V的热点事件，什么国内的某某什么丑闻，多少人去谈王岐山？又有多少人谈孟建柱？委内瑞拉、马来西亚有多大？又有多少人去关注？泰国的船翻是多少人死？为什么死？有多少人去关注？'),\n",
       " ('688-30',\n",
       "  '我当时就跟他说你完全理解错了，我们说这个马航有什么事情是不正常的，从来没说是什么医生。大家去看一看马航为什么那些人在飞机上？不仅仅是这些人，还有除了这些做移植的医生之外，还有更重要的是跟马来西亚和中国的情报关系的人在飞机上，为什么这人同时就死掉了？为什么这架飞机找不着？为什么中国跟马来西亚沟通这么多鬼鬼祟祟？所以说很多人就是误解啊，我非常不高兴那天他说这话，我说你完全是胡扯的。 所以我觉得法轮功成员们你们一定要记住，那天我说的很多，法轮功是一个非常有希望做大的一个社团，结果有些事情说过了，还有走极端了就失去了大家的信任。那天那个小伙子说那个完全是胡扯的，是不是？完全是胡扯的。美国政府他们？这不知道啊，这不知道。 金融将成为中国最大的问题，金融由于被盗国贼所控制，它这个问题就大了去了，方正证券现在它借机往下砸，跟你们说实话它砸是我的机会，方正证券内部正在转移财富，转移资源，然后往下砸，然后盗国贼在拼命的利用机会。'),\n",
       " ('719-4',\n",
       "  '在中央电视台共同关注说：飞行了100多次，150个人，大家可以看到，所有这个记录当中，这个（出示）全是机组，这面是乘客身份飞行目的地。这一架飞机从头到尾就是你姚家用，没任何人用，功能是你的，只有你飞，而且从订制到飞行到外面喷图到功能全是你的，然后你说这个飞机啊我给爆料了郭文贵说是不是姚家的？你能拿出来不是你姚家的吗？你能给我拿出来不是给姚家做的吗？你听说过一个孩子长得像他爹，然后你说这不像，这不是我儿子，谁能证明是我儿子啊？很简单！化验DNA！你能把买飞机卖飞机装飞机所有的过程你敢摊出来吗？你说不是你儿子，这DNA验证是你儿子。这飞机就你用啊，没别人用。 【话题】你回答我这个问题啊！！ 关键问题你没回答核心，海航是谁的？海航是谁的？海航怎么在你王岐山执政的5年里边怎么从1000从10个亿，变成了20万亿的控制资产，变成了2万亿的资本，这是你陈峰说的，不用我要证据链吧，痔疮帮们，你们可以上网去查，他说是2万亿，那你去查查嘛痔疮帮们。20万倍的增长和你787飞机的关系，股东贯君是谁？姚庆是谁？刘呈杰是谁？孙瑶是谁？这不你回答我这个问题啊！！ 如果你回答我是谁的海航，海航怎么变成20万倍成长？怎么从建行拿钱？怎么从海航拿钱？怎么从进出口银行拿钱？怎么从民生拿钱？怎么现在一下子就几千亿拿钱？贷什么款？怎么转出600亿美元？ 更夸张的事情，跟你王岐山家没关系，怎么把钱都转到你姚庆账号了？转到你姚明珊账号去了？转到你孙瑶账号去了？怎么都转到贯君刘呈杰账号去了？你能把这一系列的事情都否定了吗？叫共同关注！你想共同关注，新华社给这些老同志们骗这老同志吗？这个很简单的问题啊？说清楚不就完了嘛！ 【话题】这是神马逻辑啊？！！ 除了这个之外大家不要忘了一个核心，你光说787了。787上有个女孩儿，上飞机就跟人家睡觉，关键这个飞机飞行每次女孩儿飞行从来是女孩儿不跟姚明珊出现，我咋不说姚明珊跟别人睡觉呢？因为你们嫌她太老了嘛！')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_query = \"What happended to MH370 airplane? \"\n",
    "txts = text_search(txt_query, faiss_index, dict1)\n",
    "txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f9f375c-812e-4209-96d5-45e266808a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[0.99999994 0.9173957  0.90672827 0.9045879  0.9038532  0.897203\n",
      "  0.89675575 0.8966185  0.89628506 0.89613855]]\n",
      "Indices: [[    0     1  5983 21589 24700 29149 19581 18945 14504 27710]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f412e1c-307b-44c4-a0f4-cb4e58f52e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.28.1'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122cabb-2073-4b6d-bfb9-2eb17f9dfe25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
